# Generated by Claude. By Sebastian


"""
Exploratory data analysis helpers for the BirdCLEF 2023 train metadata
(``train_metadata.csv``). The module focuses on understanding label balance,
geographic coverage, audio quality, and contributor behavior.
"""
from __future__ import annotations

import ast
from pathlib import Path
from typing import Dict, List

import pandas as pd


def _parse_secondary(raw: str | List[str]) -> List[str]:
    """Convert the serialized list in ``secondary_labels`` into Python lists."""
    if isinstance(raw, list):
        return raw
    if not isinstance(raw, str) or raw.strip() == "":
        return []
    try:
        value = ast.literal_eval(raw)
        if isinstance(value, list):
            return [str(item) for item in value]
        return [str(value)]
    except (ValueError, SyntaxError):
        return [raw]


def basic_overview(df: pd.DataFrame) -> pd.DataFrame:
    """High-level dataset stats."""
    rows = [
        {"metric": "rows", "value": len(df)},
        {"metric": "columns", "value": df.shape[1]},
        {
            "metric": "memory_mb",
            "value": round(df.memory_usage(deep=True).sum() / 1_048_576, 2),
        },
        {"metric": "unique_primary_labels",
            "value": df["primary_label"].nunique()},
        {"metric": "unique_recordists", "value": df["author"].nunique()},
    ]
    geotagged = df[["latitude", "longitude"]].notna().all(axis=1).sum()
    rows.append({"metric": "geotagged_recordings", "value": int(geotagged)})
    rows.append(
        {
            "metric": "percent_geotagged",
            "value": round(geotagged / len(df) * 100, 2),
        }
    )

    duplicate_species = None
    rows.append({"metric": "duplicate_species_codes",
                "value": duplicate_species})
    rows.append(
        {"metric": "rating_mean", "value": round(
            df["rating"].mean(skipna=True), 2)}
    )
    rows.append(
        {"metric": "rating_std", "value": round(
            df["rating"].std(skipna=True), 2)}
    )
    return pd.DataFrame(rows)


def missingness(df: pd.DataFrame) -> pd.DataFrame:
    """Missing values per column with percentages."""
    missing = (
        df.isna()
        .sum()
        .reset_index()
        .rename(columns={"index": "column", 0: "missing"})
    )
    missing["missing_pct"] = (missing["missing"] / len(df) * 100).round(2)
    return missing.sort_values("missing_pct", ascending=False).reset_index(drop=True)


def cardinality(df: pd.DataFrame) -> pd.DataFrame:
    """Unique values and data type per column."""
    rows = []
    for column in df.columns:
        n_unique = df[column].nunique(dropna=True)
        rows.append(
            {
                "column": column,
                "dtype": str(df[column].dtype),
                "unique_values": int(n_unique),
                "unique_ratio": round(n_unique / len(df), 4),
            }
        )
    return pd.DataFrame(rows).sort_values("unique_values", ascending=False)


def label_distribution(df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:
    """Frequency table for primary species labels."""
    counts = (
        df["primary_label"]
        .value_counts()
        .rename_axis("primary_label")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)
    return counts.head(top_n)


def recording_type_distribution(df: pd.DataFrame) -> pd.DataFrame:
    """Distribution of recording types (song, call, etc.)."""
    counts = (
        df["type"]
        .value_counts()
        .rename_axis("type")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)
    return counts


def rating_distribution(df: pd.DataFrame) -> pd.DataFrame:
    """How the crowd-sourced quality ratings are distributed."""
    counts = (
        df["rating"]
        .value_counts()
        .sort_index()
        .rename_axis("rating")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)
    return counts


def rating_summary(df: pd.DataFrame) -> pd.DataFrame:
    """Descriptive stats for the rating column."""
    stats = df["rating"].describe().rename_axis("statistic").reset_index()
    stats = stats.rename(columns={"rating": "value"})
    stats["value"] = stats["value"].round(3)
    return stats


def author_activity(df: pd.DataFrame, top_n: int = 20) -> pd.DataFrame:
    """Top recordists by number of uploads."""
    counts = (
        df["author"]
        .value_counts()
        .rename_axis("author")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)
    return counts.head(top_n)


def license_distribution(df: pd.DataFrame) -> pd.DataFrame:
    """Distribution of Creative Commons licenses."""
    counts = (
        df["license"]
        .value_counts()
        .rename_axis("license")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)
    return counts


def geo_summary(df: pd.DataFrame) -> pd.DataFrame:
    """Min/median/max for geo coordinates."""
    geo = df[["latitude", "longitude"]].dropna()
    summary = {
        "latitude_min": geo["latitude"].min(),
        "latitude_median": geo["latitude"].median(),
        "latitude_max": geo["latitude"].max(),
        "longitude_min": geo["longitude"].min(),
        "longitude_median": geo["longitude"].median(),
        "longitude_max": geo["longitude"].max(),
    }
    return (
        pd.Series(summary)
        .round(3)
        .rename_axis("metric")
        .reset_index(name="value")
    )


def secondary_label_stats(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Distribution of secondary labels per recording and most common labels."""
    parsed = df["secondary_labels"].apply(_parse_secondary)
    counts = (
        parsed.apply(len)
        .value_counts()
        .sort_index()
        .rename_axis("secondary_label_count")
        .reset_index(name="records")
    )
    counts["records_pct"] = (counts["records"] / len(df) * 100).round(2)

    exploded = (
        pd.Series([label for labels in parsed for label in labels])
        if parsed.map(len).sum() > 0
        else pd.Series(dtype=str)
    )
    top_labels = (
        exploded.value_counts()
        .rename_axis("secondary_label")
        .reset_index(name="occurrences")
        .head(20)
    )
    if not top_labels.empty:
        top_labels["occurrences_pct"] = (
            top_labels["occurrences"] / exploded.size * 100
        ).round(2)
    else:
        top_labels["occurrences_pct"] = []
    return {
        "secondary_label_count_distribution": counts,
        "top_secondary_labels": top_labels,
    }


def run_all_edas(df: pd.DataFrame) -> Dict[str, pd.DataFrame]:
    """Generate a collection of ready-to-display EDA tables."""
    output = {
        "basic_overview": basic_overview(df),
        "missingness": missingness(df),
        "cardinality": cardinality(df),
        "label_distribution": label_distribution(df),
        "recording_type_distribution": recording_type_distribution(df),
        "rating_distribution": rating_distribution(df),
        "rating_summary": rating_summary(df),
        "author_activity": author_activity(df),
        "license_distribution": license_distribution(df),
        "geo_summary": geo_summary(df),
    }
    output.update(secondary_label_stats(df))
    return output


if __name__ == "__main__":
    print("Running the eda")
